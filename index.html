<html>
    
<body>
    
<pre>
<code>

MLA practicals for external

# practical no. 1

def calculateB(x, y, n):
    # sum of array x
    sx = sum(x)
    # sum of array y
    sy = sum(y)
    # for sum of product of x and y
    sxsy = 0
    # sum of square of x
    sx2 = 0
    for i in range(n):
        sxsy += x[i] * y[i]
        sx2 += x[i] * x[i]
    b = (n * sxsy - sx * sy) / (n * sx2 - sx * sx)
    return b

# Function to find the least regression line
def leastRegLine(X, Y, n):
    # Finding b
    b = calculateB(X, Y, n)
    meanX = sum(X) / n
    meanY = sum(Y) / n
    # Calculating a
    a = meanY - b * meanX
    # Printing regression line
    print("Regression line:")
    print("Y = ", '%.3f' % a, " + ", '%.3f' % b, "x", sep="")

# Driver code
# statistical data
X = [95, 85, 80, 70, 60]
Y = [90, 80, 70, 65, 60]
n = len(X)
leastRegLine(X, Y, n)



# practical no. 2

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report

# Load dataset
df = pd.read_csv('loan_data.csv')

# Display basic information about the dataset
print(df.head())
print(df.info())

# Plot countplot
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x="purpose", hue="not.fully.paid")
plt.xticks(rotation=45, ha='right')
plt.show()

# Convert categorical variable into dummy/indicator variables
pre_df = pd.get_dummies(df, columns=['purpose'], drop_first=True)
print(pre_df.head())

# Splitting the dataset into training and testing sets
X = pre_df.drop('not.fully.paid', axis=1)
y = pre_df['not.fully.paid']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=125)

# Train Naive Bayes classifier
model = GaussianNB()
model.fit(X_train, y_train)

# Predicting the test set results
y_pred = model.predict(X_test)

# Calculating Accuracy and F1 Score
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average="weighted")
print("Accuracy:", accuracy)
print("F1 Score:", f1)

# Plot confusion matrix
labels = ["Fully Paid", "Not Fully Paid"]
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot()
plt.show()



#practical no 2 | 2.0
    
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score

iris = datasets.load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = GaussianNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')




# practical no. 3

# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt

# Load Iris dataset
irisData = load_iris()

# Create feature and target arrays
X = irisData.data
y = irisData.target

# Split into training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Added random_state

neighbors = np.arange(1, 8)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

# Loop over K values
for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    # Compute training and test data accuracy
    train_accuracy[i] = knn.score(X_train, y_train) # Corrected indexing
    test_accuracy[i] = knn.score(X_test, y_test) # Corrected indexing

# Generate plot
plt.plot(neighbors, test_accuracy, label='Testing dataset Accuracy')
plt.plot(neighbors, train_accuracy, label='Training dataset Accuracy')
plt.legend()
plt.xlabel('n_neighbors')
plt.ylabel('Accuracy')
plt.title('KNN Accuracy for Different Values of K')
plt.show()


practical no 3 | 3.0 KNN

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Implement kNN classification
k_values = range(1, 20)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Plot accuracy versus k
plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, marker='o', linestyle='-')
plt.title('Accuracy vs. Number of Neighbors (k)')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.xticks(k_values)
plt.grid(True)
plt.show()

# Select the best k based on the plot
best_k = k_values[np.argmax(accuracies)]
print("Best k:", best_k)

# Re-train with the best k
best_knn = KNeighborsClassifier(n_neighbors=best_k)
best_knn.fit(X_train, y_train)
y_pred = best_knn.predict(X_test)

# Calculate accuracy and plot confusion matrix
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

conf_mat = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
plt.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()


practical no. 4

from sklearn.cluster import KMeans 
import numpy as np

# Input data
X = np.array([[1.713, 1.586], 
              [0.180, 1.786], 
              [0.353, 1.240], 
              [0.940, 1.566], 
              [1.486, 0.759], 
              [1.266, 1.106], 
              [1.540, 0.419], 
              [0.459, 1.799], 
              [8.773, 0.186]])

# Fit KMeans model
kmeans = KMeans(n_clusters=3, random_state=0).fit(X)

print("The input data is ")
print("VAR1 \t VAR2")
for val in X:
    print(val[0], "\t", val[1])

print("-" * 20)

# Get test data from the user
print("The Test data to predict ")

test_data = []
VAR1 = float(input("Enter Value for VAR1: "))
VAR2 = float(input("Enter Value for VAR2: "))
test_data.append(VAR1)
test_data.append(VAR2)
print("-" * 20)

# Predicting the cluster for test data
predicted_cluster = kmeans.predict([test_data])
print("The predicted cluster is:", predicted_cluster)




practical no. 5
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Input data
x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([5, 20, 14, 32, 22, 38])

# Create and fit Linear Regression model
model = LinearRegression()
model.fit(x, y)

# Coefficient of determination
r_sq = model.score(x, y)
print(f"Coefficient of determination: {r_sq}")

# Intercept and slope
print(f"Intercept: {model.intercept_}")
print(f"Slope: {model.coef_}")

# Creating a new model for comparison
new_model = LinearRegression().fit(x, y.reshape((-1, 1)))
print(f"Intercept (new model): {new_model.intercept_}")
print(f"Slope (new model): {new_model.coef_}")

# Predicting response
y_pred = model.predict(x)
print("Predicted response:")
print(y_pred)

# Predicting response using intercept and coefficients directly
y_pred_direct = model.intercept_ + model.coef_ * x
print("Predicted response (using intercept and coefficients directly):")
print(y_pred_direct)

# Generating new data and predicting response
x_new = np.arange(5).reshape((-1, 1))
y_new = new_model.predict(x_new)
print("Predicted response for new data:")
print(y_new)

# Plotting the data and regression line
plt.scatter(x, y, color='black', label='Data')
plt.plot(x, y_pred, color='blue', linewidth=2, label='Linear Regression')
plt.title('Linear Regression')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()

    

practical no. 6


from prettytable import PrettyTable

print('\nClassification using Na√Øve Bayes\n')

total_documents = int(input("Enter the Total Number of documents: "))

doc_class = []

# Collecting document text and class from the user

for i in range(total_documents):
    text = input(f"\nEnter the text of Doc-{i + 1}: ").lower()
    clas = input(f"Enter the class of Doc-{i + 1}: ")
    doc_class.append((text.split(), clas))

# Extracting unique keywords from all documents

keywords = set()

for text, _ in doc_class:
    keywords.update(text)

keywords = sorted(keywords)

# Creating a table to store word frequencies

probability_table = []

for _ in range(total_documents):
    probability_table.append([0] * len(keywords))

# Counting word frequencies for each document

for i, (text, _) in enumerate(doc_class):
    for j, keyword in enumerate(keywords):
        probability_table[i][j] = text.count(keyword)

# Displaying probability table using prettytable

keywords.insert(0, "Document ID")
keywords.append("Class")

ProbTable = PrettyTable()
ProbTable.field_names = keywords
ProbTable.title = "Probability of Documents"

for i, row in enumerate(probability_table):
    row.insert(0, i + 1)
    row.append(doc_class[i][1])
    ProbTable.add_row(row)

print(ProbTable)

# Calculating total word counts for each class

totalplus = totalneg = totalpluswords = totalnegwords = 0

for i, row in enumerate(probability_table):
    if doc_class[i][1] == "+":
        totalplus += 1
        totalpluswords += sum(row)
    else:
        totalneg += 1
        totalnegwords += sum(row)

# Removing non-relevant columns

keywords.pop()
keywords.pop(0)  # Corrected index for popping the first element

# Calculating vocabulary size
vocabulary = len(keywords)

print("\nVocabulary size:", vocabulary)
print("Total number of documents with class '+' and '-':", totalplus, totalneg)
print("Total number of words in documents with class '+' and '-':", totalpluswords, totalnegwords)




practical no. 7


import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as sch

# Generating synthetic data
X, _ = make_blobs(n_samples=200, centers=5, cluster_std=1.0, random_state=42)

# Finding the optimal number of clusters using the elbow method
wcss = []  # Initializing the list for the values of WCSS

# Using a for loop for iterations from 1 to 10
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plotting the elbow method graph
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, 11), wcss, marker='o')
plt.title("The Elbow Method Graph")
plt.xlabel("Number of clusters (k)")
plt.ylabel("WCSS (within-cluster sum of squares)")

# Generating dendrogram for hierarchical clustering
plt.subplot(1, 2, 2)
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean Distances')

plt.tight_layout()
plt.show()

# Training the KMeans model on the dataset with the optimal number of clusters
kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X)

# Visualizing the clusters
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50, alpha=0.7)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Centroids')
plt.title("Clusters of customers")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()




practical no. 8


import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()

# Convert it to a pandas DataFrame
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['variety'] = iris.target_names[iris.target]

# Splitting the dataset into features (X) and target variable (y)
X = iris.data
y = iris.target

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Implementing Support Vector Machine (SVM) Algorithm
clf = SVC(kernel='rbf')  # Using the radial basis function kernel
clf.fit(X_train, y_train)

# Making predictions on the test set
y_pred = clf.predict(X_test)

# Evaluating the classifier accuracy
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
</code>
</pre>
</body>

</html>
